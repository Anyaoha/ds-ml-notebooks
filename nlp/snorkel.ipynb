{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snorkel\n",
    "\n",
    "- Walking through how to implement Snorkel to generate training data with labels. For this example, we want to identify mentions of *spouses* in a corpus of news articles. There are a total of 50 documents in this corpus but only the first 10 will be considered due to run times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First,instantiate SnorkelSession, and this will manage a connection to the sqlite3 db automatically; otherwise it's mentioned to use Postgres db.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the Corpus for Pre-Processing. Note, the objective again is to identify the mention of spouses read in the documents and parse as tab separated value (tsv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.parser import TSVDocPreprocessor\n",
    "doc_preprocessor = TSVDocPreprocessor('data/articles.tsv',max_docs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Example....\n",
    "**0001_c9843f4e-9c43-4eca-9665-37aef78f5ea3**\tThe Duke of Cambridge has thrown his support behind an organisation's fight against bullying - and listed an enviable support network. \\n \\nWilliam wrote down Catherine, Harry, father, grandmother, grandfather and an extra - his dog Lupo - when he joined a Diana Fund trainee session for anti-bullying ambassadors. \\n \\nFifty youngsters from across the country were set the \"high five\" task of naming five people they would turn to for help with verbal, physical or cyber abuse. \\n \\nThe Duke was given a large cardboard hand to fill in and named his immediate family - better known as wife Kate, Prince Harry, the Prince of Wales, the Queen and the Duke of Edinburgh - before signing the palm with his name......\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 343 ms, sys: 27.8 ms, total: 371 ms\n",
      "Wall time: 3.76 s\n"
     ]
    }
   ],
   "source": [
    "#split docs into sentences and tokens \n",
    "from snorkel.parser import CorpusParser\n",
    "corpus_parse = CorpusParser()\n",
    "% time corpus_parse.apply(doc_preprocessor)\n",
    "\n",
    "#you can run in parallel for faster run times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Documents in Corpus: 10\\m\n",
      "Total Number of Sentences:184\n"
     ]
    }
   ],
   "source": [
    "#Check the number of documents created --> this case should be 10\n",
    "#Count the number of sentences in the documents\n",
    "#Query the SQLITE database\n",
    "\n",
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "print \"Total Number of Documents in Corpus: {}\\m\".format(session.query(Document).count())\n",
    "print \"Total Number of Sentences:{}\".format(session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tables created in SQL**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "candidate       feature         gold_label_key  parameter       sentence    context         feature_key     label           prediction      span        document        gold_label      label_key       prediction_key stable_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to SQLITE and check if created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1, 0, u\"The Duke of Cambridge has thrown his support behind an organisation's fight against bullying - and listed an enviable support network.\", <read-write buffer ptr 0x10f5f15d0, size 279 at 0x10f5f1590>, <read-write buffer ptr 0x10e65d250, size 54 at 0x10e65d210>, <read-write buffer ptr 0x10d33f310, size 267 at 0x10d33f2d0>, <read-write buffer ptr 0x10f5d2f10, size 218 at 0x10f5d2ed0>, <read-write buffer ptr 0x10e14fef0, size 168 at 0x10e14feb0>, <read-write buffer ptr 0x10d7f0520, size 54 at 0x10d7f04e0>, <read-write buffer ptr 0x10ce80d90, size 272 at 0x10ce80d50>, <read-write buffer ptr 0x10f937cd0, size 77 at 0x10f937c90>, <read-write buffer ptr 0x10f937bb0, size 77 at 0x10f937b70>)\n",
      "\n",
      "\n",
      "(12, 1, 1, u'\\\\n \\\\nWilliam wrote down Catherine, Harry, father, grandmother, grandfather and an extra - his dog Lupo - when he joined a Diana Fund trainee session for anti-bullying ambassadors.', <read-write buffer ptr 0x10f6e1930, size 383 at 0x10f6e18f0>, <read-write buffer ptr 0x10f937d60, size 78 at 0x10f937d20>, <read-write buffer ptr 0x10f6e2598, size 376 at 0x10f6e2558>, <read-write buffer ptr 0x10f63d670, size 315 at 0x10f63d630>, <read-write buffer ptr 0x10f61f8c0, size 239 at 0x10f61f880>, <read-write buffer ptr 0x10f937df0, size 78 at 0x10f937db0>, <read-write buffer ptr 0x10cfb8470, size 444 at 0x10cfb8430>, <read-write buffer ptr 0x10f6e6350, size 113 at 0x10f6e6310>, <read-write buffer ptr 0x10f6e6408, size 113 at 0x10f6e63c8>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "try:# try connecting to the snorkel db\n",
    "    db = sqlite3.connect('snorkel.db')\n",
    "    c = db.cursor()\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "#Run query to return the sentences\n",
    "c.execute(\"SELECT * FROM sentence LIMIT 2\")\n",
    "for row in c.fetchall():\n",
    "    print(row)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract spouse relations** or what snorkel calls \"candidates\" from the corpus (i.e. doc_preprocessor) and determine whether the candidates are pairs of people that would be classified as maried or not (yes/no). Note, the term candidate is what we want to make a prediction on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining a Schema for the candidate**\n",
    "- Creating a binary spouse relation mention by connecting two span objects (i.e. the given sequences of start and end positions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "try: #try creating the spouse table if not exist\n",
    "    Spouse = candidate_subclass('Spouse', ['person1', 'person2'])\n",
    "except: # pass if spouse table created\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check that the spouse table has been created**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "candidate       feature_key     label_key       sentence      \n",
    "context         gold_label      parameter       span          \n",
    "document        gold_label_key  prediction      spouse        \n",
    "feature         label           prediction_key  stable_label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table Schema**\n",
    "- Columns: person1 and person2 created"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CREATE TABLE spouse (\n",
    "\tid INTEGER NOT NULL, \n",
    "\tperson1_id INTEGER, \n",
    "\tperson1_cid INTEGER, \n",
    "\tperson2_id INTEGER, \n",
    "\tperson2_cid INTEGER, \n",
    "\tPRIMARY KEY (id), \n",
    "\tUNIQUE (person1_id, person2_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract candidate spouse relation mentions from the corpus**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import PersonMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *ngrams*: Models that assign probabilities to sequences of words are refered to as Language Models. One example is Ngram, which is a sequence of \"N\" words. \n",
    "- For this example, only three consecutive words (i.e. \"The Duke of\") is considered. This model estimates the probability of the last word given the previous words and will assign a probability to the seqeunce of three words in this case. \n",
    "- However, for this example, we want to extract candidate objects for \"Spouse\" type that are tagged as \"Person\" for Ngrams = 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngrams = Ngrams(n_max=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *PersonMatcher* -- Looks for all n_grams such that matches the names of **people** identified by CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "person_matcher = PersonMatcher(longest_match_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*CandidateExtractor* = Look for all n_grams up to 3 words long and filters the candidates by this criteria. Candidates are then stored in the Spouse table of the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cand_extractor = CandidateExtractor(Spouse, \n",
    "    [ngrams, ngrams], [person_matcher, person_matcher],\n",
    "    symmetric_relations=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filter out the sentences based on number of people mentioned:\n",
    "def num_people(sentence):\n",
    "    active_sequence = False\n",
    "    count = 0\n",
    "    for tag in sentence.ner_tags:\n",
    "        if tag == \"Person\" and not active_sequence:\n",
    "            active_sequence = True\n",
    "            count += 1\n",
    "        elif tag != \"Person\" and active_sequence:\n",
    "            active_sequence = False\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Data into Train/Dev/Test sets:** (i.e. 90%/5%/5% and will split in non-random ordertp preserve the splits already labled as person). Note, the train/dev/test groups will be labeled as 0/1/2.\n",
    "\n",
    "- In this example, we will filiter out sentences that mention at least 5 people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Document\n",
    "\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "\n",
    "#initialize sets\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "test_sents  = set()\n",
    "\n",
    "#specifiy splits\n",
    "splits = (0.8, 0.9) \n",
    "for i,doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if num_people(s) < 5:\n",
    "            if i < splits[0] * ld:\n",
    "                #print(i)\n",
    "                train_sents.add(s)\n",
    "            elif i < splits[1] * ld:\n",
    "                dev_sents.add(s)\n",
    "            else:\n",
    "                test_sents.add(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Candidate Extractor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the *cand_extractor* function to the training sentences. Again, here we are trying to extract feature names for \"person\" in the training set. You can executet this using a parallelism parameter if using databases other than SQLITE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 568 ms, sys: 82.9 ms, total: 651 ms\n",
      "Wall time: 614 ms\n"
     ]
    }
   ],
   "source": [
    "%time cand_extractor.apply(train_sents, split=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, get the candidates that were just extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates in training set: 80\n"
     ]
    }
   ],
   "source": [
    "#Run query to return the sentences\n",
    "train_cands = session.query(Spouse).filter(Spouse.split == 0).all()\n",
    "print(\"Number of candidates in training set: {}\".format(len(train_cands)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try:# try connecting to the snorkel db\n",
    "#     db = sqlite3.connect('snorkel.db')\n",
    "#     c = db.cursor()\n",
    "# except Exception as e:\n",
    "#     print(str(e))\n",
    "    \n",
    "# c.execute(\"SELECT COUNT('type') FROM Spouse WHERE('type') == 0\")\n",
    "# for row in c.fetchone():\n",
    "#      print(\"Number of Candidates: {}\".format(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspect the Candidates:** Note, the objective is to Maximize the Recall -> which is the ratio of the predicted true positives over the actual true positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# # NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# # You should ignore this!\n",
    "# import os\n",
    "# if 'CI' not in os.environ:\n",
    "#     sv = SentenceNgramViewer(train_cands[:300], session)\n",
    "# else:\n",
    "#     sv = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Candidates are tuples of Context-type objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span(\"Andres\", sentence=61, chars=[62,67], words=[14,14])\n",
      "Span(\"Enoch\", sentence=86, chars=[136,140], words=[25,25])\n",
      "Span(\"Enoch\", sentence=86, chars=[136,140], words=[25,25])\n",
      "Span(\"Enoch\", sentence=86, chars=[136,140], words=[25,25])\n",
      "Span(\"Nelson Johnson\", sentence=86, chars=[364,377], words=[70,71])\n",
      "Span(\"Nelson Johnson\", sentence=86, chars=[364,377], words=[70,71])\n",
      "Span(\"Johnson\", sentence=86, chars=[150,156], words=[29,29])\n",
      "Span(\"Steven Zaillian\", sentence=77, chars=[61,75], words=[14,15])\n",
      "Span(\"Steven Zaillian\", sentence=77, chars=[61,75], words=[14,15])\n",
      "Span(\"Steven Zaillian\", sentence=77, chars=[61,75], words=[14,15])\n",
      "Span(\"Frank Lucas\", sentence=77, chars=[225,235], words=[46,47])\n",
      "Span(\"Frank Lucas\", sentence=77, chars=[225,235], words=[46,47])\n",
      "Span(\"Ridley Scott\", sentence=77, chars=[285,296], words=[57,58])\n",
      "Span(\"Serzh Sargsyan\", sentence=112, chars=[51,64], words=[11,12])\n",
      "Span(\"Denzel Washington\", sentence=78, chars=[49,65], words=[9,10])\n",
      "Span(\"Tony Camonte\", sentence=83, chars=[82,93], words=[25,26])\n",
      "Span(\"Mann\", sentence=82, chars=[10,13], words=[3,3])\n",
      "Span(\"Mann\", sentence=82, chars=[10,13], words=[3,3])\n",
      "Span(\"Mann\", sentence=82, chars=[10,13], words=[3,3])\n",
      "Span(\"Christian Bale\", sentence=82, chars=[117,130], words=[23,24])\n",
      "Span(\"Christian Bale\", sentence=82, chars=[117,130], words=[23,24])\n",
      "Span(\"Depp\", sentence=82, chars=[111,114], words=[21,21])\n",
      "Span(\"Cody Jarrett\", sentence=80, chars=[18,29], words=[3,4])\n",
      "Span(\"Daniel Day-Lewis\", sentence=92, chars=[102,117], words=[19,20])\n",
      "Span(\"Nicholas Pileggi\", sentence=89, chars=[65,80], words=[21,22])\n",
      "Span(\"Martin Scorsese\", sentence=87, chars=[97,111], words=[16,17])\n",
      "Span(\"Lupo\", sentence=12, chars=[98,101], words=[21,21])\n",
      "Span(\"Lupo\", sentence=12, chars=[98,101], words=[21,21])\n",
      "Span(\"Catherine\", sentence=12, chars=[24,32], words=[6,6])\n",
      "Span(\"Kate\", sentence=14, chars=[112,115], words=[24,24])\n",
      "Span(\"Kate\", sentence=14, chars=[112,115], words=[24,24])\n",
      "Span(\"Duke\", sentence=14, chars=[9,12], words=[4,4])\n",
      "Span(\"Eliot Ness\", sentence=85, chars=[176,185], words=[26,27])\n",
      "Span(\"Eliot Ness\", sentence=85, chars=[176,185], words=[26,27])\n",
      "Span(\"Eliot Ness\", sentence=85, chars=[176,185], words=[26,27])\n",
      "Span(\"Eliot Ness\", sentence=85, chars=[176,185], words=[26,27])\n",
      "Span(\"Eliot Ness\", sentence=85, chars=[176,185], words=[26,27])\n",
      "Span(\"Oscar Fraley\", sentence=85, chars=[145,156], words=[21,22])\n",
      "Span(\"Oscar Fraley\", sentence=85, chars=[145,156], words=[21,22])\n",
      "Span(\"Oscar Fraley\", sentence=85, chars=[145,156], words=[21,22])\n",
      "Span(\"Oscar Fraley\", sentence=85, chars=[145,156], words=[21,22])\n",
      "Span(\"Capone\", sentence=85, chars=[229,234], words=[37,37])\n",
      "Span(\"Capone\", sentence=85, chars=[229,234], words=[37,37])\n",
      "Span(\"Capone\", sentence=85, chars=[229,234], words=[37,37])\n",
      "Span(\"Brian De Palma-directed\", sentence=85, chars=[78,100], words=[12,14])\n",
      "Span(\"Brian De Palma-directed\", sentence=85, chars=[78,100], words=[12,14])\n",
      "Span(\"Robert De Niro\", sentence=85, chars=[276,289], words=[45,47])\n",
      "Span(\"Steven Zaillian\", sentence=91, chars=[193,207], words=[41,42])\n",
      "Span(\"Steven Zaillian\", sentence=91, chars=[193,207], words=[41,42])\n",
      "Span(\"Steven Zaillian\", sentence=91, chars=[193,207], words=[41,42])\n",
      "Span(\"Kenneth Lonergan\", sentence=91, chars=[210,225], words=[44,45])\n",
      "Span(\"Kenneth Lonergan\", sentence=91, chars=[210,225], words=[44,45])\n",
      "Span(\"Scorsese\", sentence=91, chars=[306,313], words=[60,60])\n",
      "Span(\"Baby Face Nelson\", sentence=81, chars=[316,331], words=[70,72])\n",
      "Span(\"Baby Face Nelson\", sentence=81, chars=[316,331], words=[70,72])\n",
      "Span(\"Baby Face Nelson\", sentence=81, chars=[316,331], words=[70,72])\n",
      "Span(\"Baby Face Nelson\", sentence=81, chars=[316,331], words=[70,72])\n",
      "Span(\"Baby Face Nelson\", sentence=81, chars=[316,331], words=[70,72])\n",
      "Span(\"J. Edgar Hoover\", sentence=81, chars=[239,253], words=[54,56])\n",
      "Span(\"J. Edgar Hoover\", sentence=81, chars=[239,253], words=[54,56])\n",
      "Span(\"J. Edgar Hoover\", sentence=81, chars=[239,253], words=[54,56])\n",
      "Span(\"J. Edgar Hoover\", sentence=81, chars=[239,253], words=[54,56])\n",
      "Span(\"Mann\", sentence=81, chars=[39,42], words=[13,13])\n",
      "Span(\"Mann\", sentence=81, chars=[39,42], words=[13,13])\n",
      "Span(\"Mann\", sentence=81, chars=[39,42], words=[13,13])\n",
      "Span(\"John Dillinger\", sentence=81, chars=[278,291], words=[62,63])\n",
      "Span(\"John Dillinger\", sentence=81, chars=[278,291], words=[62,63])\n",
      "Span(\"Bryan Burrough\", sentence=81, chars=[106,119], words=[27,28])\n",
      "Span(\"Francis\", sentence=79, chars=[168,174], words=[38,38])\n",
      "Span(\"Francis\", sentence=79, chars=[168,174], words=[38,38])\n",
      "Span(\"James Cagney\", sentence=79, chars=[54,65], words=[16,17])\n",
      "Span(\"Barack Obama\", sentence=111, chars=[85,96], words=[15,16])\n",
      "Span(\"Johnny Depp\", sentence=74, chars=[25,35], words=[5,6])\n",
      "Span(\"Klaus Andres\", sentence=53, chars=[42,53], words=[10,11])\n",
      "Span(\"Dick Lehr\", sentence=75, chars=[170,178], words=[34,35])\n",
      "Span(\"Dick Lehr\", sentence=75, chars=[170,178], words=[34,35])\n",
      "Span(\"Dick Lehr\", sentence=75, chars=[170,178], words=[34,35])\n",
      "Span(\"Gerard O'Neill\", sentence=75, chars=[184,197], words=[37,38])\n",
      "Span(\"Gerard O'Neill\", sentence=75, chars=[184,197], words=[37,38])\n",
      "Span(\"Scott Cooper\", sentence=75, chars=[30,41], words=[5,6])\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "#There should be 80 Candidates\n",
    "count = 0\n",
    "for c in train_cands:\n",
    "    count +=1\n",
    "    print(c.person1)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The hierarchy of context objects in Snorkel is:**\n",
    "    - Documents\n",
    "    - Sentences\n",
    "    - Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0004_4233a2bb-4611-4993-9fe4-da863fe62488\n",
      "\n",
      "\n",
      "Sentence(Document 0004_4233a2bb-4611-4993-9fe4-da863fe62488, 4, u'\"Crazy Heart\" writer-director Scott Cooper helmed the drama, based on the 2000 book Black Mass: The True Story of an Unholy Alliance Between the FBI and the Irish Mob by Dick Lehr and Gerard O\\'Neill, which tells the sordid tale of Whitey Bulger, a merciless South Boston mobster who collaborated with the Feds to bring down his Italian rivals.')\n",
      "\n",
      "\n",
      "Span(\"Scott Cooper\", sentence=75, chars=[30,41], words=[5,6])\n",
      "Span(\"Whitey Bulger\", sentence=75, chars=[231,243], words=[46,47])\n"
     ]
    }
   ],
   "source": [
    "span1 = c.get_contexts()[0]\n",
    "span2 = c.get_contexts()[1]\n",
    "\n",
    "print span.get_parent().get_parent()\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print span.get_parent()\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(span1)\n",
    "print(span2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of getting Span, tokens, and tag**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerard O'Neill\n",
      "[u'Gerard', u\"O'Neill\"]\n",
      "[u'NNP', u'NNP']\n"
     ]
    }
   ],
   "source": [
    "print span.get_span()\n",
    "print span.get_attrib_tokens()\n",
    "print(span.get_attrib_tokens('pos_tags')) # Proper Nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeat for development and testing corpora** \n",
    "\n",
    "- Repeat same process above. First, load in the corpus object, collect the sentence objects, and then run the CandidateExtractor on both the development and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "1\n",
      "Number of candidates: 4\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "2\n",
      "Number of candidates: 11\n",
      "CPU times: user 263 ms, sys: 68.1 ms, total: 331 ms\n",
      "Wall time: 288 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, sents in enumerate([dev_sents, test_sents]):\n",
    "    cand_extractor.apply(sents, split=i+1)\n",
    "    print(i+1)\n",
    "    print \"Number of candidates:\", session.query(Spouse).filter(Spouse.split == i+1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating or Loading Evaluation Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall that Snorkel is utilized to enable training of machine learning models without the task of hand-labeling training data for classifcation types of problems. However, a small amount of labeled data to help develop and evaluate the application is stil required.\n",
    "\n",
    "Required is two small labeled data sets:\n",
    "\n",
    "- A development set, which can be a subset of the training set, which we use to help guide in this process. \n",
    "\n",
    "\n",
    "- A test set to evaluate the final performance against. **Important:** You should get someone that is not involved in development of your application to label the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Candidates: 4\n"
     ]
    }
   ],
   "source": [
    "dev_cands = session.query(Spouse).filter(Spouse.split == 1).all()\n",
    "print(\"Testing Candidates: {}\".format(len(dev_cands)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Candidates: 11\n"
     ]
    }
   ],
   "source": [
    "test_cands = session.query(Spouse).filter(Spouse.split == 2).all()\n",
    "print(\"Testing Candidates: {}\".format(len(test_cands)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**External annotations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File gold_labels.tsv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-455-d1cdd2079585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mload_external_annotations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_external_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mload_external_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpouse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotator_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gold'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mload_external_annotations.py\u001b[0m in \u001b[0;36mload_external_labels\u001b[0;34m(session, candidate_class, annotator_name)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4175)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8440)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File gold_labels.tsv does not exist"
     ]
    }
   ],
   "source": [
    "from load_external_annotations import load_external_labels\n",
    "load_external_labels(session, Spouse, annotator_name='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "#if 'CI' not in os.environ:\n",
    "    #sv = SentenceNgramViewer(dev_cands, session)3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {
    "c040f9a4449841a4a1e9ca9695a34278": {
     "views": [
      {
       "cell_index": 50
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
