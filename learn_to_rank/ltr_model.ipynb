{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c18714e-6fab-4bed-945a-4640c8c01b28",
   "metadata": {},
   "source": [
    "# Learning to Rank\n",
    "______\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "**About**\n",
    "The datasets are machine learning data, in which queries and urls are represented by IDs. The datasets consist of feature vectors extracted from query-url pairs along with relevance judgment labels:\n",
    "- Reference: https://www.microsoft.com/en-us/research/project/mslr/\n",
    "\n",
    "**Labels**\n",
    "- The relevance judgments take 5 values from 0 (irrelevant) to 4 (perfectly relevant). \n",
    "- The larger value the relevance label has, the more relevant the query-url pair is.\n",
    "\n",
    "**Features**:\n",
    "- A query-url pair is represented by a 136-dimensional feature vector\n",
    "- Each row corresponds to a query-url pair.\n",
    "- First column is relevance label of the pair, \n",
    "- Second column is query id, and the following columns are features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddfc4b3d-82fd-4e27-b2e7-e7d1e6ab49b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172269e9-f981-4279-8e4d-ee16c39dea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_dir\", default=None, type=str)\n",
    "    parser.add_argument(\"--num_leaves\", type=int, default=10)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=0.20)\n",
    "    parser.add_argument(\"--reg_lambda\", type=float, default=2)\n",
    "    return parser\n",
    "\n",
    "sys.argv = ['-','--data_dir','data']\n",
    "args = get_args().parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a94edaf4-f688-4b8d-9ab6-4ccdded8fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path):\n",
    "    dfs = {\n",
    "        \"train\": pd.read_csv(f\"{data_path}/train.txt\", delimiter=\" \"),\n",
    "        \"valid\": pd.read_csv(f\"{data_path}/vali.txt\", delimiter=\" \"),\n",
    "        \"test\": pd.read_csv(f\"{data_path}/test.txt\", delimiter=\" \"),\n",
    "    }\n",
    "\n",
    "    for df in dfs.values():\n",
    "        df.columns = np.arange(len(df.columns))\n",
    "        # delete columns where all data is missing\n",
    "        df.drop(columns=df.columns[df.isna().all()].tolist(), inplace=True)\n",
    "        \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1a116cfc-fd63-45a4-ba8b-bb83a604c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dfs):\n",
    "    \n",
    "    split = {}\n",
    "    split[\"X_train\"] = dfs[\"train\"].iloc[:, 1:]\n",
    "    split[\"X_valid\"] = dfs[\"valid\"].iloc[:, 1:]\n",
    "    split[\"X_test\"] = dfs[\"test\"].iloc[:, 1:]\n",
    "    \n",
    "    # In order to use the Light GBM framework, we need to \n",
    "    # create variables group_train and group_vali, which contain\n",
    "    # number of examples for each query ID. This will allow LGBMRanker \n",
    "    # to group examples by query during training.\n",
    "\n",
    "    # Train\n",
    "    g = split[\"X_train\"].groupby(by=1)\n",
    "    size = g.size()\n",
    "    group_train = size.to_list()\n",
    "\n",
    "    # Validation\n",
    "    g = split[\"X_valid\"].groupby(by=1)\n",
    "    size = g.size()\n",
    "    group_valid = size.to_list()\n",
    "\n",
    "    # Relevance Labels\n",
    "    y_train = dfs[\"train\"].iloc[:, 0]\n",
    "    y_valid = dfs[\"valid\"].iloc[:, 0]\n",
    "    y_test = dfs[\"test\"].iloc[:, 0]\n",
    "    \n",
    "    return split[\"X_train\"], split[\"X_valid\"], split[\"X_test\"], group_train, group_valid, y_train, y_test, y_valid, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "048ad7a6-92a9-441f-8833-bb3f3c69ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    \"\"\" Preprocess Train/Validation/Test Dataset \n",
    "    \n",
    "    Task:\n",
    "    1. Remove the irrelevant information at the beginning of each feature value\n",
    "    2. Drop the query ID column since it is not a feature\n",
    "    \"\"\"\n",
    "    X = X.astype(str).applymap(lambda x: x.split(\":\")[-1]).astype(float)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f030884a-87f0-40c7-aeb5-dcdbd8877b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(args, X_train, X_valid, y_train, y_valid, group_train, group_valid):\n",
    "    \"\"\" Train LightGBM Ranker Model \"\"\" \n",
    "    \n",
    "    gbm = lgb.LGBMRanker(\n",
    "        n_estimators=10000,\n",
    "        num_leaves=args.num_leaves,\n",
    "        learning_rate=args.learning_rate,\n",
    "        reg_lambda=args.reg_lambda,\n",
    "    )\n",
    "\n",
    "    gbm.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        group=group_train,\n",
    "        eval_group=[group_valid],\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        early_stopping_rounds=150,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    return gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3dbf7a05-ab48-48e6-9695-ef6eebb7da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evalute model results\n",
    "    \n",
    "    Compute Normalized Discounted Cumulative Gain.\n",
    "\n",
    "    Sum the true scores ranked in the order induced by the predicted scores,\n",
    "    after applying a logarithmic discount. Then divide by the best possible\n",
    "    score (Ideal DCG, obtained for a perfect ranking) to obtain a score between\n",
    "    0 and 1.\n",
    "\n",
    "    This ranking metric yields a high value if true labels are ranked high by\n",
    "    `y_score`.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # ground truth by sorting it according to our model's predictions.\n",
    "    true_relevance = y_test.sort_values(ascending=False)\n",
    "\n",
    "    # prediction\n",
    "    test_pred = model.predict(X_test)\n",
    "\n",
    "    # Save Results\n",
    "    y_test = pd.DataFrame({\"relevance_score\": y_test, \"predicted_ranking\": test_pred})\n",
    "    relevance_score = y_test.sort_values(\"predicted_ranking\", ascending=False)\n",
    "    \n",
    "    # Normalized Discounted Cumulative Gain\n",
    "    score = ndcg_score([true_relevance.to_numpy()], [relevance_score[\"relevance_score\"].to_numpy()])\n",
    "    print(f\"nDCG score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e71965-6093-4b27-acc2-a6642e5ca98a",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "23bb9221-0999-4a2d-ae06-837526ceaef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "dfs = get_data(args.data_dir)\n",
    "\n",
    "# Split Train/Test\n",
    "X_train, X_valid, X_test, group_train, group_valid, y_train, y_valid, y_test = train_test_split(dfs)\n",
    "\n",
    "# Preprocess \n",
    "X_train = preprocess(X_train)\n",
    "X_valid = preprocess(X_valid)\n",
    "X_test = preprocess(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "15972d6e-e452-42e7-8684-b217c4234559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = train_model(args, X_train, X_valid, y_train, y_valid, group_train, group_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1408558f-7fb0-4b41-81a2-7023662ae01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "evaluate_model(model,X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pluaral",
   "language": "python",
   "name": "pluaral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
